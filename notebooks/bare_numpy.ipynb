{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inlinea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "climb = pd.read_pickle('../scraping/test_data/_climb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_location = '/Users/adam/RouteRobot/scraping/vocab.txt'\n",
    "vocab = pd.read_csv(vocab_location, header=None)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmb = climb.iloc[0]\n",
    "cmb['sparse_tfidf'].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_matrix(cmb, route_description_weight = 1.618):\n",
    "    \"\"\" Add sparse representations of route and area descriptions together \"\"\"\n",
    "    \n",
    "    if pd.isnull(cmb['parent_sparse_tfidf']):\n",
    "        if pd.isnull(cmb['sparse_tfidf']):\n",
    "            return float('NaN')\n",
    "        else:\n",
    "            return cmb['sparse_tfidf']\n",
    "    else:\n",
    "        area = np.array(cmb['parent_sparse_tfidf'].todense())\n",
    "        route = np.array(cmb['sparse_tfidf'].todense()) * route_description_weight\n",
    "        added = area + route\n",
    "        # L2 normalize again\n",
    "        text_vector = added / norm(added, 2)\n",
    "        return csr_matrix(text_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105708962      (0, 2737)\\t0.173051326107\\n  (0, 2736)\\t0.13...\n",
       "105718711      (0, 37)\\t0.182443060869\\n  (0, 108)\\t0.10999...\n",
       "105718771      (0, 89)\\t0.228553873599\\n  (0, 137)\\t0.18316...\n",
       "105738019      (0, 0)\\t0.0588036851687\\n  (0, 3)\\t0.0930428...\n",
       "105738022      (0, 0)\\t0.0611109882915\\n  (0, 8)\\t0.0584010...\n",
       "105738025      (0, 83)\\t0.0588035216612\\n  (0, 88)\\t0.05765...\n",
       "105738028      (0, 3)\\t0.118968763599\\n  (0, 137)\\t0.239816...\n",
       "105738031      (0, 11)\\t0.128489088824\\n  (0, 42)\\t0.076926...\n",
       "105738034      (0, 0)\\t0.0592546475342\\n  (0, 8)\\t0.0566270...\n",
       "105738040      (0, 0)\\t0.059802839263\\n  (0, 8)\\t0.05715093...\n",
       "105738043      (0, 15)\\t0.0713729635833\\n  (0, 87)\\t0.06933...\n",
       "105738047      (0, 0)\\t0.0589695178429\\n  (0, 8)\\t0.0563545...\n",
       "105738050      (0, 303)\\t0.116815005687\\n  (0, 334)\\t0.1389...\n",
       "105738053      (0, 18)\\t0.117681494463\\n  (0, 195)\\t0.06285...\n",
       "105738056      (0, 11)\\t0.0778628382375\\n  (0, 42)\\t0.04661...\n",
       "105738059      (0, 11)\\t0.0793604478316\\n  (0, 42)\\t0.04751...\n",
       "105738062      (0, 13)\\t0.125281092016\\n  (0, 16)\\t0.143054...\n",
       "105738065      (0, 8)\\t0.252138713908\\n  (0, 9)\\t0.03689325...\n",
       "105738068      (0, 195)\\t0.0704015087027\\n  (0, 390)\\t0.260...\n",
       "105738071      (0, 18)\\t0.111144059228\\n  (0, 37)\\t0.117626...\n",
       "105738074      (0, 72)\\t0.145653516771\\n  (0, 114)\\t0.12540...\n",
       "105738077      (0, 83)\\t0.0545863801272\\n  (0, 88)\\t0.05352...\n",
       "105738080      (0, 80)\\t0.117001762448\\n  (0, 123)\\t0.11761...\n",
       "105738083      (0, 5)\\t0.127015480811\\n  (0, 226)\\t0.144914...\n",
       "105738086      (0, 18)\\t0.105015140685\\n  (0, 87)\\t0.130202...\n",
       "105738089      (0, 18)\\t0.11934308151\\n  (0, 20)\\t0.1557733...\n",
       "105738092      (0, 59)\\t0.0543319926403\\n  (0, 83)\\t0.06202...\n",
       "105738095      (0, 83)\\t0.0567456643083\\n  (0, 88)\\t0.05563...\n",
       "105738098      (0, 22)\\t0.0769151772492\\n  (0, 59)\\t0.30157...\n",
       "105738101      (0, 59)\\t0.0181961840756\\n  (0, 137)\\t0.0674...\n",
       "                                   ...                        \n",
       "111786366      (0, 0)\\t0.107896423431\\n  (0, 37)\\t0.0732012...\n",
       "111790320      (0, 0)\\t0.104797259807\\n  (0, 37)\\t0.0710986...\n",
       "111790372      (0, 0)\\t0.109149515841\\n  (0, 37)\\t0.0740514...\n",
       "111790419      (0, 0)\\t0.109566991443\\n  (0, 37)\\t0.0743346...\n",
       "111790694      (0, 34)\\t0.197268721653\\n  (0, 42)\\t0.123136...\n",
       "111792662      (0, 18)\\t0.067596252943\\n  (0, 82)\\t0.188621...\n",
       "111792670      (0, 298)\\t0.171935587618\\n  (0, 480)\\t0.2119...\n",
       "111792716      (0, 165)\\t0.224375062337\\n  (0, 298)\\t0.2952...\n",
       "111792989      (0, 137)\\t0.220694125623\\n  (0, 242)\\t0.2019...\n",
       "111793013      (0, 89)\\t0.056897169844\\n  (0, 165)\\t0.14370...\n",
       "111800443      (0, 13)\\t0.201488606637\\n  (0, 69)\\t0.141441...\n",
       "111803200      (0, 207)\\t0.179691392997\\n  (0, 367)\\t0.3387...\n",
       "111807801      (0, 28)\\t0.215344059557\\n  (0, 278)\\t0.07129...\n",
       "111807931      (0, 130)\\t0.10762203373\\n  (0, 137)\\t0.06097...\n",
       "111807942      (0, 16)\\t0.308103392934\\n  (0, 59)\\t0.161489...\n",
       "111808896      (0, 16)\\t0.0621747942283\\n  (0, 27)\\t0.12222...\n",
       "111809156      (0, 130)\\t0.0655194358902\\n  (0, 442)\\t0.071...\n",
       "111809474      (0, 28)\\t0.104607956585\\n  (0, 59)\\t0.057772...\n",
       "111810541      (0, 42)\\t0.194555108156\\n  (0, 130)\\t0.24247...\n",
       "111810627      (0, 602)\\t0.292587685857\\n  (0, 753)\\t0.1865...\n",
       "111812311      (0, 126)\\t0.115102424111\\n  (0, 189)\\t0.3790...\n",
       "111813201      (0, 161)\\t0.249824113591\\n  (0, 185)\\t0.1746...\n",
       "111815360      (0, 42)\\t0.103046150008\\n  (0, 47)\\t0.145599...\n",
       "111815653      (0, 4)\\t0.0571478965481\\n  (0, 88)\\t0.037448...\n",
       "111820471      (0, 89)\\t0.178875988506\\n  (0, 137)\\t0.14334...\n",
       "111820480      (0, 7)\\t0.32738645514\\n  (0, 89)\\t0.11140791...\n",
       "111820483      (0, 7)\\t0.203126280762\\n  (0, 149)\\t0.194234...\n",
       "111820499      (0, 7)\\t0.432631947258\\n  (0, 207)\\t0.190811...\n",
       "111820510      (0, 7)\\t0.551719095598\\n  (0, 203)\\t0.099849...\n",
       "111823652      (0, 185)\\t0.221245722284\\n  (0, 416)\\t0.1655...\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.linalg import norm\n",
    "from scipy.sparse import csr_matrix\n",
    "climb.apply(combine_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'todense'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-11a73c603160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcmb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclimb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0marea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parent_sparse_tfidf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mroute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sparse_tfidf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mroute_description_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0madded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marea\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mroute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# L2 normalize again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'todense'"
     ]
    }
   ],
   "source": [
    "if \n",
    "area = np.array(cmb['parent_sparse_tfidf'].todense())\n",
    "route = np.array(cmb['sparse_tfidf'].todense()) * route_description_weight\n",
    "added = area + route\n",
    "# L2 normalize again\n",
    "text_vector = added / norm(added, 2)\n",
    "return csr_matrix(text_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_pattern = re.compile(r'(?:A[KLRZ]|C[AOT]|D[CE]|FL|GA|HI|I[ADLN]|K[SY]|LA|M[ADEINOST]|N[CDEHJMVY]|O[HKR]|PA|RI|S[CD]|T[NX]|UT|V[AT]|W[AIVY])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match at 0x10ef2b4a8>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isa_state_pattern.search('HI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_table = pd.read_csv('../scraping/state_table.csv', index_col=0)\n",
    "\n",
    "def index_climbs_by_state(climb):\n",
    "    \"\"\" Make filtering by state faster by pre-indexing \"\"\"\n",
    "    state_index = {}\n",
    "    for state in state_table['abbreviation']:\n",
    "        proper_state_name = state_table['name'][state_table['abbreviation'] == state].iloc[0]\n",
    "        loc = climb['state_name'] == proper_state_name\n",
    "        state_index[state] = loc\n",
    "    return state_index\n",
    "state_index = index_climbs_by_state(climb)\n",
    "# pickle.dump(state_index, open('state_index','wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def detect_geography(tokens, state_pattern):\n",
    "    states = []\n",
    "    for tk in tokens:\n",
    "        if state_pattern.search(tk):\n",
    "            states.append(tk)\n",
    "    if len(states) > 0:\n",
    "        return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states = detect_geography(['overhung', 'crimps', 'evening', 'AL', 'fwjehfb', 'UT'], state_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_geography(climb, state_index, states = None):\n",
    "    \"\"\" return boolean column of climbs in those states \"\"\"\n",
    "    include = ~climb.any(1)\n",
    "    if states:\n",
    "        for state in states:\n",
    "            include = include | state_index[state]\n",
    "    return include\n",
    "# filter_geography(climb, state_index, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-5819d3ce6aee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclimb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/adam/RouteRobot/venv/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2667\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2669\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "climb.dtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_grade_map(climb, rating_system = ['rateHueco','rateYDS']):\n",
    "    grade_map = {}\n",
    "    for label in rating_system:\n",
    "        grpd = climb.groupby(label)['grade'].mean()\n",
    "        grade_map.update(dict(zip(grpd.index, grpd.ravel())))\n",
    "    return grade_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create_grade_map(climb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def map_grades(climb):\n",
    "#    grpd = climb.groupby(by=['rateHueco'])\n",
    "#    grade = grpd.mean()\n",
    "    #\n",
    "#    return dict(zip(hueco, grade))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grade = grpd.mean()\n",
    "    \n",
    "    return dict(zip(hueco, grade))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def get_sparse_X(text_segments, vocab):\n",
    "    \"\"\" vocab words from description\n",
    "    returns scipy matrix\n",
    "    \"\"\"\n",
    "    # lemmatize, tokenize, vectorize text\n",
    "    tfidf = TfidfVectorizer(\n",
    "        vocabulary=vocab,\n",
    "        strip_accents = 'unicode', lowercase=True, ngram_range=(1,2),\n",
    "        norm='l2', sublinear_tf=False, smooth_idf=True, use_idf=True\n",
    "        )\n",
    "    X = tfidf.fit_transform(text_segments)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = get_sparse_X(climb['description'], vocab)\n",
    "N = float(X.shape[0])\n",
    "# nnz = (X != 0).sum(0)[0].tolist()[0]\n",
    "# calculate IDF\n",
    "# dump IDF as dictionary\n",
    "climb['sparse_tfidf'] = [X[i] for i in range(X.shape[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_idf_weights(X, vocab):\n",
    "    # count nonzeros by column\n",
    "    nnz = np.array((X != 0).sum(0))\n",
    "    # avoids div by zero error\n",
    "    nnz[nnz == 0] = 1\n",
    "    # weight by inverse document frequency\n",
    "    idf = np.log(N / nnz)\n",
    "\n",
    "    idf_lookup = dict(zip(vocab, idf[0].tolist()))\n",
    "\n",
    "    return idf_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "idf_lookup = calculate_idf_weights(X, vocab)\n",
    "pickle.dump(idf_lookup, open('idf', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print min(nnz[0])\n",
    "nnz[nnz == 0] = 1\n",
    "print min(nnz[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2746"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dict(zip(vocab, Df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105717367    weekends  chalked  incredible  corner  form  r...\n",
       "Name: keyword, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#climb[climb['nickname'] == 'genericcrack']\n",
    "climb[climb.index=='105717367']['keyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LIMIT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    vocabulary = vocab,\n",
    "    strip_accents = 'unicode', lowercase=True, norm='l2',\n",
    "    sublinear_tf=True, smooth_idf=True, use_idf=True\n",
    "    )\n",
    "text_segments = climb['description'].tolist()\n",
    "X = tfidf.fit_transform(text_segments)#[:LIMIT]\n",
    "#climb = climb[:LIMIT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69949, 3674)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(r'tfidf_matrix.pickle', 'wb') as ouput_file:\n",
    "    pickle.dump(X, ouput_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-d75a0cedee7a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-d75a0cedee7a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    NEED CLIMB INDEX\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "NEED CLIMB INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(r'tfidf_matrix.pickle', 'rb') as input_file:\n",
    "    X = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = dirty_query.strip().lower()\n",
    "unigrams = query.split(' ')\n",
    "bigrams = [ b[0]+' '+b[1] for l in [query] for b in zip(l.split(' ')[:-1], l.split(' ')[1:]) ]\n",
    "tokens = [ t for t in (unigrams + bigrams) if t in vocab ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def inverse_document_frequency(word, X, vocab):\n",
    "    \"\"\" Given word and tfidf_matrix find its inverse document frequency\n",
    "    http://nlp.stanford.edu/IR-book/html/htmledition/inverse-document-frequency-1.html\n",
    "    \"\"\"\n",
    "    # count of times all words appear\n",
    "    nnz = (X != 0).sum(0)[0].tolist()[0]\n",
    "    # get this word's count\n",
    "    # max avoids div by zero error\n",
    "    Df = max(1,nnz[vocab.index(word)])\n",
    "    # total number of documents in the corpus\n",
    "    N = X.shape[0]\n",
    "    idf = math.log(N / Df)\n",
    "    return idf\n",
    "\n",
    "def score_tokens(tokens, X, vocab):\n",
    "    \"\"\" Assign tfidf scores to tokens \"\"\"\n",
    "    word_scores = list()\n",
    "    for word in set(tokens):\n",
    "        idf = inverse_document_frequency(word, X, vocab)\n",
    "        tf = tokens.count(word)\n",
    "        word_scores.append(tf * idf)\n",
    "    zpd = dict(zip(list(set(tokens)), word_scores))\n",
    "    return zpd\n",
    "\n",
    "def sanitize(dirty_query):\n",
    "    \"\"\" Preprocess query string \"\"\"\n",
    "    query = dirty_query.strip().lower()\n",
    "    unigrams = query.split(' ')\n",
    "    bigrams = [ b[0]+' '+b[1] for l in [query] for b in zip(l.split(' ')[:-1], l.split(' ')[1:]) ]\n",
    "    tokens = [ t for t in (unigrams + bigrams) if t in vocab ]\n",
    "    return tokens\n",
    "\n",
    "def score_query(dirty_query, X, vocab):\n",
    "    \"\"\" Find best matching descriptions \"\"\"\n",
    "    \n",
    "    tokens = sanitize(dirty_query)\n",
    "    # weight tokens by tfidf score\n",
    "    keyword = score_tokens(tokens, X, vocab)\n",
    "    \n",
    "    # represent tokens with a vector in vocabulary space\n",
    "    text_vector = np.zeros((1,len(vocab)))\n",
    "    for word, score in keyword.items():\n",
    "        text_vector[0][vocab.index(word)] = score\n",
    "\n",
    "    # measure similarity of vector to mp climb descriptions\n",
    "    cos_sim = (text_vector * X.T)[0]\n",
    "    scores = (cos_sim / max(cos_sim)).tolist()\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dirty_query = 'fun'\n",
    "ideal = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the loc grammar seems to be import to pandas\n",
    "display = climb.loc[:,('name','grade','feet','staraverage','starvotes','keyword')] # FUCKING LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display['text_similarity_score'] = score_query(dirty_query, X, vocab)\n",
    "# display = display.sort_values('text_similarity_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scores look lognormal\n",
    "# pd.Series([s for s in scores if s > 0]).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def grade_scorer(grade, ideal):\n",
    "    return 1 - abs(ideal - grade)\n",
    "\n",
    "def score_grade(grade, ideal):\n",
    "    grade_score = map(grade_scorer, grade, itertools.repeat(ideal, len(grade)))\n",
    "    return grade_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display['grade_score'] = score_grade(grade = display['grade'], ideal = ideal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display['staraverage_score'] = map(math.log, display['staraverage'])\n",
    "#display['starvotes_score'] = map(math.log, display['starvotes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display['best'] = display[['staraverage_score','grade_score','text_similarity_score']].sum(1)\n",
    "display = display[display['starvotes'] > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display.sort_values('best', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if col in ['aid','alpine','boulder','sport','trad','ice']:\n",
    "#     return ideal * grade\n",
    "# elif col in ['feet', 'pitches', 'grade']:\n",
    "#     return 1 - abs(ideal - grade)\n",
    "# elif col in ['staraverage', 'starvotes']:\n",
    "#     return grade"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
